{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11860099,"sourceType":"datasetVersion","datasetId":7452518},{"sourceId":11887475,"sourceType":"datasetVersion","datasetId":7471598}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Question-1 - Defining Dataset(vocab) and seq2seq model classes","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nimport wandb\nwandb.login(key=\"f659082c2b19bf3ffaaceceb36c1e280541f6b11\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class tsvtokenizer(Dataset):\n    def __init__(self, tsv_file, src_vocab=None, tgt_vocab=None, max_len=32, build_vocab=False):\n        # Read TSV file - the fields are separated by tabs, and we're interested in the first two columns\n        # Each line has: native_text, roman_text, frequency\n        try:\n            df = pd.read_csv(tsv_file, sep='\\t', header=None, \n                             names=['native', 'roman', 'freq'], \n                             usecols=[0, 1], dtype=str)\n            print(f\"Successfully loaded {len(df)} entries from {tsv_file}\")\n            \n            # Fill NA values to prevent errors\n            df['native'] = df['native'].fillna('')\n            df['roman'] = df['roman'].fillna('')\n            \n            # Create pairs of (roman, native) for transliteration\n            self.pairs = list(zip(df['roman'], df['native']))\n            print(f\"Sample data: {self.pairs[:2]}\")\n        except Exception as e:\n            print(f\"Error loading dataset: {e}\")\n            # Create an empty dataset as fallback\n            self.pairs = [('', '')]\n            \n        self.max_len = max_len\n        # Build or use provided vocabulary\n        if build_vocab:\n            # Start vocabulary with special tokens including <sos> and <eos>\n            self.src_vocab = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n            self.tgt_vocab = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n            self._build_vocab()\n        else:\n            self.src_vocab, self.tgt_vocab = src_vocab, tgt_vocab\n            \n    def _build_vocab(self):\n        # Build character-level vocabulary from the dataset\n        for src, tgt in self.pairs:\n            for ch in src:\n                if ch not in self.src_vocab: \n                    self.src_vocab[ch] = len(self.src_vocab)\n            for ch in tgt:\n                if ch not in self.tgt_vocab: \n                    self.tgt_vocab[ch] = len(self.tgt_vocab)\n        print(f\"Built vocabularies - Source: {len(self.src_vocab)}, Target: {len(self.tgt_vocab)}\")\n    \n    def __len__(self): \n        return len(self.pairs)\n    \n    def __getitem__(self, idx):\n        src, tgt = self.pairs[idx]\n        \n        # Convert characters to indices with bounds checking and add start/end tokens\n        # Source sequence (input)\n        src_idxs = [self.src_vocab['<sos>']]  # Start token\n        for ch in src:\n            idx = self.src_vocab.get(ch, self.src_vocab['<unk>'])\n            if idx >= len(self.src_vocab):\n                idx = self.src_vocab['<unk>']  # Safety check\n            src_idxs.append(idx)\n        src_idxs.append(self.src_vocab['<eos>'])  # End token\n            \n        # Target sequence (output)\n        tgt_idxs = [self.tgt_vocab['<sos>']]  # Start token\n        for ch in tgt:\n            idx = self.tgt_vocab.get(ch, self.tgt_vocab['<unk>'])\n            if idx >= len(self.tgt_vocab):\n                idx = self.tgt_vocab['<unk>']  # Safety check\n            tgt_idxs.append(idx)\n        tgt_idxs.append(self.tgt_vocab['<eos>'])  # End token\n        \n        # Make sure we don't exceed max_len (accounting for <sos> and <eos> tokens)\n        if len(src_idxs) > self.max_len:\n            # Keep <sos>, truncate middle, and keep <eos>\n            src_idxs = [src_idxs[0]] + src_idxs[1:self.max_len-1] + [src_idxs[-1]]\n        if len(tgt_idxs) > self.max_len:\n            # Keep <sos>, truncate middle, and keep <eos>\n            tgt_idxs = [tgt_idxs[0]] + tgt_idxs[1:self.max_len-1] + [tgt_idxs[-1]]\n        \n        # Add padding\n        pad_src = [self.src_vocab['<pad>']] * (self.max_len - len(src_idxs))\n        pad_tgt = [self.tgt_vocab['<pad>']] * (self.max_len - len(tgt_idxs))\n        \n        # Combine with padding\n        src_idxs = src_idxs + pad_src\n        tgt_idxs = tgt_idxs + pad_tgt\n        \n        # Make sure padding index is valid\n        assert self.src_vocab['<pad>'] < len(self.src_vocab), \"Padding index out of bounds for source vocab\"\n        assert self.tgt_vocab['<pad>'] < len(self.tgt_vocab), \"Padding index out of bounds for target vocab\"\n        \n        return torch.tensor(src_idxs, dtype=torch.long), torch.tensor(tgt_idxs, dtype=torch.long)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---- Model Definition ----\nclass Seq2Seq(nn.Module):\n    def __init__(self, cfg, src_vocab_size, tgt_vocab_size):\n        super().__init__()\n        # Make sure vocab sizes are valid\n        assert src_vocab_size > 0, f\"Invalid source vocabulary size: {src_vocab_size}\"\n        assert tgt_vocab_size > 0, f\"Invalid target vocabulary size: {tgt_vocab_size}\"\n        \n        # Define model hyperparameters\n        self.embed_dim = cfg.embed_dim\n        self.hidden_dim = cfg.hidden_dim\n        self.cell_type = cfg.cell_type\n        self.enc_layers = cfg.enc_layers\n        self.dec_layers = cfg.dec_layers\n        \n        # Initialize embeddings\n        self.src_emb = nn.Embedding(src_vocab_size, cfg.embed_dim, padding_idx=0)\n        self.tgt_emb = nn.Embedding(tgt_vocab_size, cfg.embed_dim, padding_idx=0)\n        \n        # Cell type selection\n        if cfg.cell_type == 'RNN':\n            cell = nn.RNN\n        elif cfg.cell_type == 'GRU':\n            cell = nn.GRU\n        elif cfg.cell_type == 'LSTM':\n            cell = nn.LSTM\n        else:\n            raise ValueError(f\"Unsupported cell type: {cfg.cell_type}\")\n        \n        # Define dropout rates\n        enc_dr = cfg.dropout if cfg.enc_layers > 1 else 0.0\n        dec_dr = cfg.dropout if cfg.dec_layers > 1 else 0.0\n        # Initialize encoder and decoder\n        self.encoder = cell(cfg.embed_dim, cfg.hidden_dim,\n                               num_layers=cfg.enc_layers, batch_first=True, dropout=enc_dr)\n        self.decoder = cell(cfg.embed_dim, cfg.hidden_dim,\n                               num_layers=cfg.dec_layers, batch_first=True, dropout=dec_dr)\n            \n        # Output projection\n        self.fc = nn.Linear(cfg.hidden_dim, tgt_vocab_size)\n        \n        print(f\"Model initialized: {cfg.cell_type} with {cfg.enc_layers} encoder layers, \"\n              f\"{cfg.dec_layers} decoder layers, {cfg.embed_dim} embedding dim, \"\n              f\"{cfg.hidden_dim} hidden dim\")\n\n    def forward(self, src, tgt):\n        batch_size = src.size(0)\n        src_len = src.size(1)\n        tgt_len = tgt.size(1)\n        device = src.device\n        # Check for out of bounds indices\n        if src.max() >= self.src_emb.num_embeddings:\n            print(f\"Warning: Source index {src.max().item()} is out of bounds for vocab size {self.src_emb.num_embeddings}\")\n            # Clamp indices to valid range\n            src = torch.clamp(src, 0, self.src_emb.num_embeddings - 1)\n            \n        if tgt.max() >= self.tgt_emb.num_embeddings:\n            print(f\"Warning: Target index {tgt.max().item()} is out of bounds for vocab size {self.tgt_emb.num_embeddings}\")\n            # Clamp indices to valid range\n            tgt = torch.clamp(tgt, 0, self.tgt_emb.num_embeddings - 1)\n        \n        # Forward through encoder\n        enc_in = self.src_emb(src)\n        \n        try:\n            if self.cell_type == 'LSTM':\n                _, (h_n, c_n) = self.encoder(enc_in)\n                \n                # Adapt hidden state dimensions for different layer counts\n                if self.enc_layers != self.dec_layers:\n                    # Initialize new hidden and cell states with correct dimensions\n                    h_new = torch.zeros(self.dec_layers, batch_size, self.hidden_dim, device=device)\n                    c_new = torch.zeros(self.dec_layers, batch_size, self.hidden_dim, device=device)\n                    # Copy as many layers as possible\n                    copy_layers = min(self.enc_layers, self.dec_layers)\n                    h_new[:copy_layers] = h_n[:copy_layers]\n                    c_new[:copy_layers] = c_n[:copy_layers]\n                    \n                    h_n, c_n = h_new, c_new\n                \n                # Forward through decoder with adjusted hidden states\n                dec_in = self.tgt_emb(tgt)\n                dec_out, _ = self.decoder(dec_in, (h_n, c_n))\n                \n            else:  # RNN or GRU\n                _, h_n = self.encoder(enc_in)\n                \n                # Adapt hidden state dimensions for different layer counts\n                if self.enc_layers != self.dec_layers:\n                    h_new = torch.zeros(self.dec_layers, batch_size, self.hidden_dim, device=device)\n                    copy_layers = min(self.enc_layers, self.dec_layers)\n                    h_new[:copy_layers] = h_n[:copy_layers]\n                    h_n = h_new\n                \n                # Forward through decoder with adjusted hidden state\n                dec_in = self.tgt_emb(tgt)\n                dec_out, _ = self.decoder(dec_in, h_n)\n                \n            # Output projection\n            return self.fc(dec_out)\n            \n        except Exception as e:\n            print(f\"Error in forward pass: {e}\")\n            # Return dummy tensor on error\n            return torch.zeros(batch_size, tgt.size(1), self.tgt_emb.num_embeddings, device=device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate(model, src, max_len=32, eos_idx=3, sos_idx=2, device=\"cuda\"):\n    \"\"\"\n    Generate target sequence using greedy decoding.\n    \n    Args:\n        model: Trained Seq2Seq model\n        src: Source sequence tensor [batch_size, seq_len]\n        max_len: Maximum length of generated sequence\n        eos_idx: Index of <eos> token in target vocabulary\n        sos_idx: Index of <sos> token in target vocabulary\n        device: Device to run inference on\n        \n    Returns:\n        List of generated sequences\n    \"\"\"\n    model.eval()  # Set model to evaluation mode\n    batch_size = src.size(0)\n    \n    with torch.no_grad():\n        # Encode source sequence\n        enc_in = model.src_emb(src)\n        \n        if model.cell_type == 'LSTM':\n            _, (h_n, c_n) = model.encoder(enc_in)\n            \n            # Adapt hidden state dimensions for decoder if needed\n            if model.enc_layers != model.dec_layers:\n                h_new = torch.zeros(model.dec_layers, batch_size, model.hidden_dim, device=device)\n                c_new = torch.zeros(model.dec_layers, batch_size, model.hidden_dim, device=device)\n                copy_layers = min(model.enc_layers, model.dec_layers)\n                h_new[:copy_layers] = h_n[:copy_layers]\n                c_new[:copy_layers] = c_n[:copy_layers]\n                h_n, c_n = h_new, c_new\n                \n            hidden = (h_n, c_n)\n        else:  # RNN or GRU\n            _, h_n = model.encoder(enc_in)\n            \n            # Adapt hidden state dimensions for decoder if needed\n            if model.enc_layers != model.dec_layers:\n                h_new = torch.zeros(model.dec_layers, batch_size, model.hidden_dim, device=device)\n                copy_layers = min(model.enc_layers, model.dec_layers)\n                h_new[:copy_layers] = h_n[:copy_layers]\n                h_n = h_new\n                \n            hidden = h_n\n        \n        # Start with <sos> tokens for each sequence in batch\n        current_token = torch.full((batch_size, 1), sos_idx, dtype=torch.long, device=device)\n        \n        # Store generated sequences\n        generated_sequences = torch.zeros(batch_size, max_len, dtype=torch.long, device=device)\n        generated_sequences[:, 0] = sos_idx\n        \n        # Track if sequence is completed (has generated EOS)\n        completed = torch.zeros(batch_size, dtype=torch.bool, device=device)\n        \n        # Generate one token at a time\n        for i in range(1, max_len):\n            # Embed current token\n            dec_in = model.tgt_emb(current_token)\n            \n            # Pass through decoder\n            if model.cell_type == 'LSTM':\n                dec_out, hidden = model.decoder(dec_in, hidden)\n            else:\n                dec_out, hidden = model.decoder(dec_in, hidden)\n                \n            # Get output probabilities and select most likely token\n            logits = model.fc(dec_out)\n            probs = F.softmax(logits, dim=-1)\n            next_token = torch.argmax(probs, dim=-1)\n            \n            # Store token in output sequence\n            generated_sequences[:, i] = next_token.squeeze(1)\n            \n            # Mark sequences that generated EOS\n            completed = completed | (next_token.squeeze(1) == eos_idx)\n            \n            # Stop if all sequences have generated EOS\n            if completed.all():\n                break\n                \n            # Update current token for next iteration\n            # Only update tokens for sequences that haven't completed yet\n            current_token = next_token\n            \n            # For completed sequences, we'll continue the loop but their tokens\n            # won't matter since we'll post-process the output\n            \n        # Post-process sequences - ensure all have EOS and nothing after\n        for b in range(batch_size):\n            # Find first EOS token (if any)\n            eos_positions = (generated_sequences[b] == eos_idx).nonzero(as_tuple=True)[0]\n            \n            if len(eos_positions) > 0:\n                # Get the position of the first EOS\n                first_eos = eos_positions[0].item()\n                \n                # If EOS isn't the last token, zero out everything after it\n                if first_eos < max_len - 1:\n                    generated_sequences[b, first_eos+1:] = 0\n            else:\n                # If no EOS found, append it at the end\n                generated_sequences[b, -1] = eos_idx\n                \n    return generated_sequences\n\n\ndef decode_predictions(sequences, idx_to_char, eos_idx=3):\n    \"\"\"\n    Convert token indices to characters and trim after first <eos>.\n    \n    Args:\n        sequences: Tensor of token indices [batch_size, seq_len]\n        idx_to_char: Dictionary mapping indices to characters\n        eos_idx: Index of <eos> token\n        \n    Returns:\n        List of decoded strings\n    \"\"\"\n    batch_size = sequences.size(0)\n    decoded = []\n    \n    for b in range(batch_size):\n        chars = []\n        for idx in sequences[b]:\n            idx = idx.item()\n            if idx == eos_idx:\n                chars.append('<eos>')\n                break  # Stop at first <eos>\n            if idx != 0:  # Skip padding\n                chars.append(idx_to_char.get(idx, '<unk>'))\n        \n        decoded.append(''.join(chars))\n    \n    return decoded","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, loader, criterion, optimizer, device):\n    \"\"\"\n    Train the model for one epoch.\n    \n    Args:\n        model: The Seq2Seq model\n        loader: DataLoader with training data\n        criterion: Loss function\n        optimizer: Optimizer\n        device: Device to use for computation\n        \n    Returns:\n        Average loss for the epoch\n    \"\"\"\n    model.train()\n    total_loss = 0.0\n    num_batches = len(loader)\n    \n    for batch_idx, (src, tgt) in enumerate(loader):\n        try:\n            # Move data to device\n            src, tgt = src.to(device), tgt.to(device)\n            \n            # Verify indices are within vocabulary size range\n            src_max = src.max().item()\n            tgt_max = tgt.max().item()\n            if src_max >= model.src_emb.num_embeddings or tgt_max >= model.tgt_emb.num_embeddings:\n                print(f\"Batch {batch_idx}/{num_batches}: Invalid indices - \"\n                      f\"Source max: {src_max}, Target max: {tgt_max}, \"\n                      f\"Source vocab: {model.src_emb.num_embeddings}, Target vocab: {model.tgt_emb.num_embeddings}\")\n                # Skip this batch\n                continue\n            \n            # Zero gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            logits = model(src, tgt[:, :-1])\n            \n            # Reshape for loss calculation\n            logits_flat = logits.reshape(-1, logits.size(-1))\n            tgt_flat = tgt[:, 1:].reshape(-1)\n            \n            # Compute loss\n            loss = criterion(logits_flat, tgt_flat)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n                \n        except Exception as e:\n            print(f\"Error in batch {batch_idx}/{num_batches}: {e}\")\n            continue\n    \n    return total_loss / num_batches","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def decode_batch(batch_sequences, idx_to_char, eos_idx=3, join=True):\n    \"\"\"\n    Decode a batch of token sequences back to text.\n    \n    Args:\n        batch_sequences: Tensor of token indices [batch_size, seq_len]\n        idx_to_char: Dictionary mapping indices to characters\n        eos_idx: Index of <eos> token\n        join: Whether to join characters into strings\n        \n    Returns:\n        List of decoded strings or character lists\n    \"\"\"\n    batch_size = batch_sequences.size(0)\n    decoded = []\n    \n    for b in range(batch_size):\n        chars = []\n        for idx in batch_sequences[b]:\n            idx = idx.item()\n            # Skip <sos> (assumed to be at the start)\n            if len(chars) == 0 and idx == 2:  # <sos> token\n                continue\n                \n            # Stop at <eos>\n            if idx == eos_idx:\n                break\n                \n            # Skip padding\n            if idx == 0:\n                continue\n                \n            chars.append(idx_to_char.get(idx, '<unk>'))\n        \n        if join:\n            decoded.append(''.join(chars))\n        else:\n            decoded.append(chars)\n    \n    return decoded\n\n\ndef transliterate(model, text, src_vocab, tgt_vocab, device, max_len=32):\n    \"\"\"\n    Transliterate a single text input.\n    \n    Args:\n        model: Trained Seq2Seq model\n        text: Input text string\n        src_vocab: Source vocabulary (char -> idx)\n        tgt_vocab: Target vocabulary (char -> idx)\n        device: Device to run inference on\n        max_len: Maximum length of generated sequence\n        \n    Returns:\n        Transliterated text\n    \"\"\"\n    model.eval()\n    \n    # Create inverse vocab for decoding\n    idx_to_char = {idx: char for char, idx in tgt_vocab.items()}\n    \n    # Get indices for special tokens\n    sos_idx = src_vocab.get('<sos>', 2)  # Default to 2 if not in vocab\n    eos_idx = src_vocab.get('<eos>', 3)  # Default to 3 if not in vocab\n    \n    # Convert input text to tensor\n    indices = [src_vocab.get('<sos>', 2)]  # Start with <sos>\n    for ch in text:\n        indices.append(src_vocab.get(ch, src_vocab.get('<unk>', 1)))\n    indices.append(src_vocab.get('<eos>', 3))  # End with <eos>\n    \n    # Pad to max_len\n    if len(indices) < max_len:\n        indices += [src_vocab.get('<pad>', 0)] * (max_len - len(indices))\n    else:\n        # If too long, keep <sos>, as much content as fits, and <eos>\n        indices = [indices[0]] + indices[1:max_len-1] + [indices[-1]]\n    \n    # Convert to tensor and add batch dimension\n    src_tensor = torch.tensor([indices], dtype=torch.long).to(device)\n    \n    # Generate transliteration\n    with torch.no_grad():\n        output = generate(\n            model, \n            src_tensor,\n            max_len=max_len,\n            eos_idx=tgt_vocab.get('<eos>', 3),\n            sos_idx=tgt_vocab.get('<sos>', 2),\n            device=device\n        )\n        \n        # Decode the output\n        result = decode_batch(output, idx_to_char, eos_idx=tgt_vocab.get('<eos>', 3))[0]\n    \n    return result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Metric functions","metadata":{}},{"cell_type":"code","source":"def exact_match_accuracy(model, loader, device, tgt_vocab, sos_idx=2, eos_idx=3):\n    \"\"\"\n    Compute exact match accuracy for transliteration.\n    \n    Args:\n        model: The Seq2Seq model\n        loader: DataLoader with evaluation data\n        device: Device to use for computation\n        tgt_vocab: Target vocabulary (char -> idx)\n        sos_idx: Index of the start-of-sequence token\n        eos_idx: Index of the end-of-sequence token\n        \n    Returns:\n        Exact match accuracy percentage\n    \"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    \n    # Create reverse vocabulary mapping for decoding\n    idx_to_char = {idx: char for char, idx in tgt_vocab.items()}\n    \n    with torch.no_grad():\n        for src, tgt in loader:\n            # Move data to device\n            src, tgt = src.to(device), tgt.to(device)\n            batch_size = src.size(0)\n            \n            # Use our generate function for inference\n            predictions = generate(\n                model,\n                src,\n                max_len=tgt.size(1),  # Use same max length as target\n                eos_idx=eos_idx,\n                sos_idx=sos_idx,\n                device=device\n            )\n            \n            # Convert predictions and targets to strings for comparison\n            for i in range(batch_size):\n                # Process target sequence (skip <sos>, stop at <eos>)\n                target_seq = []\n                for idx in tgt[i, 1:]:  # Skip <sos>\n                    idx = idx.item()\n                    if idx == eos_idx:  # Stop at <eos>\n                        break\n                    if idx != 0:  # Skip padding\n                        target_seq.append(idx)\n                \n                # Process predicted sequence (skip <sos>, stop at <eos>)\n                pred_seq = []\n                for idx in predictions[i, 1:]:  # Skip <sos>\n                    idx = idx.item()\n                    if idx == eos_idx:  # Stop at <eos>\n                        break\n                    if idx != 0:  # Skip padding\n                        pred_seq.append(idx)\n                \n                # Convert indices to characters\n                target_text = ''.join([idx_to_char.get(idx, '') for idx in target_seq])\n                pred_text = ''.join([idx_to_char.get(idx, '') for idx in pred_seq])\n                \n                # Check if prediction exactly matches target\n                if target_text == pred_text:\n                    correct += 1\n                total += 1\n    \n    # Return accuracy as a percentage\n    return 100.0 * correct / total if total > 0 else 0.0\n\n\ndef compute_metrics(model, loader, device, src_vocab, tgt_vocab, sos_idx=2, eos_idx=3):\n    \"\"\"\n    Compute comprehensive evaluation metrics for the model.\n    \n    Args:\n        model: The Seq2Seq model\n        loader: DataLoader with evaluation data\n        device: Device to use for computation\n        tgt_vocab: Target vocabulary (char -> idx)\n        sos_idx: Index of the start-of-sequence token\n        eos_idx: Index of the end-of-sequence token\n        \n    Returns:\n        Dictionary with multiple evaluation metrics\n    \"\"\"\n    model.eval()\n    \n    # Metrics\n    exact_matches = 0\n    total_samples = 0\n    char_correct = 0\n    char_total = 0\n    \n    # For more detailed analysis\n    results = []\n    \n    # Create reverse vocabulary mapping for decoding\n    idx_to_char = {idx: char for char, idx in tgt_vocab.items()}\n    \n    with torch.no_grad():\n        for src, tgt in loader:\n            # Move data to device\n            src, tgt = src.to(device), tgt.to(device)\n            batch_size = src.size(0)\n            \n            # Use our generate function for inference\n            predictions = generate(\n                model,\n                src,\n                max_len=tgt.size(1),  # Use same max length as target\n                eos_idx=eos_idx,\n                sos_idx=sos_idx,\n                device=device\n            )\n            \n            # Process each sample in the batch\n            for i in range(batch_size):\n                # Convert source to text\n                src_text = \"\"\n                for idx in src[i]:\n                    idx = idx.item()\n                    if idx == sos_idx:\n                        continue  # Skip <sos>\n                    if idx == eos_idx:\n                        break  # Stop at <eos>\n                    if idx != 0:  # Skip padding\n                        src_char = next((char for char, vidx in src_vocab.items() if vidx == idx), '')\n                        src_text += src_char\n                \n                # Process target sequence (skip <sos>, stop at <eos>)\n                target_seq = []\n                for idx in tgt[i, 1:]:  # Skip <sos>\n                    idx = idx.item()\n                    if idx == eos_idx:  # Stop at <eos>\n                        break\n                    if idx != 0:  # Skip padding\n                        target_seq.append(idx)\n                \n                # Process predicted sequence (skip <sos>, stop at <eos>)\n                pred_seq = []\n                for idx in predictions[i, 1:]:  # Skip <sos>\n                    idx = idx.item()\n                    if idx == eos_idx:  # Stop at <eos>\n                        break\n                    if idx != 0:  # Skip padding\n                        pred_seq.append(idx)\n                \n                # Convert indices to characters\n                target_text = ''.join([idx_to_char.get(idx, '') for idx in target_seq])\n                pred_text = ''.join([idx_to_char.get(idx, '') for idx in pred_seq])\n                \n                # Check exact match\n                is_exact_match = target_text == pred_text\n                if is_exact_match:\n                    exact_matches += 1\n                \n                # Count character-level matches\n                min_len = min(len(target_text), len(pred_text))\n                char_correct += sum(1 for j in range(min_len) if target_text[j] == pred_text[j])\n                char_total += len(target_text)\n                \n                # Store result for this sample\n                results.append({\n                    'source': f\"<sos>{src_text}<eos>\",\n                    'target': f\"{target_text}<eos>\",\n                    'prediction': f\"{pred_text}<eos>\",\n                    'is_correct': is_exact_match\n                })\n                \n                total_samples += 1\n    \n    # Calculate metrics\n    exact_match_acc = (exact_matches / total_samples) * 100 if total_samples > 0 else 0\n    char_acc = (char_correct / char_total) * 100 if char_total > 0 else 0\n    \n    return {\n        'exact_match_accuracy': exact_match_acc,\n        'character_accuracy': char_acc,\n        'results': results[:10]\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def eval_model(model, loader, criterion, device, sos_idx=2, eos_idx=3):\n    \"\"\"\n    Evaluate the model on validation data.\n    \n    Args:\n        model: The Seq2Seq model\n        loader: DataLoader with validation data\n        criterion: Loss function\n        device: Device to use for computation\n        sos_idx: Index of <sos> token\n        eos_idx: Index of <eos> token\n        \n    Returns:\n        Dictionary with evaluation metrics\n    \"\"\"\n    model.eval()\n    total_loss = 0.0\n    num_batches = len(loader)\n    \n    # For computing accuracy\n    total_correct = 0\n    total_tokens = 0\n    \n    # For sequence accuracy\n    correct_sequences = 0\n    total_sequences = 0\n    \n    with torch.no_grad():\n        for batch_idx, (src, tgt) in enumerate(loader):\n            try:\n                src, tgt = src.to(device), tgt.to(device)\n                batch_size = src.size(0)\n                \n                # Forward pass for loss computation\n                logits = model(src, tgt[:, :-1])\n                \n                # Reshape for loss calculation\n                logits_flat = logits.reshape(-1, logits.size(-1))\n                tgt_flat = tgt[:, 1:].reshape(-1)\n                \n                # Compute loss\n                loss = criterion(logits_flat, tgt_flat)\n                total_loss += loss.item()\n                \n                # Use our generate function for inference\n                predictions = generate(\n                    model,\n                    src,\n                    max_len=tgt.size(1),  # Use same max length as target\n                    eos_idx=eos_idx,\n                    sos_idx=sos_idx,\n                    device=device\n                )\n                \n                # Compute accuracy metrics\n                for b in range(batch_size):\n                    # Compare each predicted token with target\n                    # Skip the first token (<sos>) for evaluation\n                    pred_seq = predictions[b, 1:]  # Skip <sos>\n                    tgt_seq = tgt[b, 1:]  # Skip <sos>\n                    \n                    # Find position of first <eos> in each sequence\n                    pred_eos_pos = (pred_seq == eos_idx).nonzero(as_tuple=True)[0]\n                    tgt_eos_pos = (tgt_seq == eos_idx).nonzero(as_tuple=True)[0]\n                    \n                    # Get effective lengths (up to first <eos> or full length)\n                    pred_len = pred_eos_pos[0].item() + 1 if len(pred_eos_pos) > 0 else len(pred_seq)\n                    tgt_len = tgt_eos_pos[0].item() + 1 if len(tgt_eos_pos) > 0 else len(tgt_seq)\n                    \n                    # Get the sequences up to the first <eos> or full length\n                    pred_trimmed = pred_seq[:pred_len]\n                    tgt_trimmed = tgt_seq[:tgt_len]\n                    \n                    # Compare each token\n                    min_len = min(len(pred_trimmed), len(tgt_trimmed))\n                    correct_tokens = (pred_trimmed[:min_len] == tgt_trimmed[:min_len]).sum().item()\n                    \n                    total_correct += correct_tokens\n                    total_tokens += len(tgt_trimmed)\n                    \n                    # Check if the whole sequence is correct\n                    if len(pred_trimmed) == len(tgt_trimmed) and correct_tokens == len(tgt_trimmed):\n                        correct_sequences += 1\n                    \n                    total_sequences += 1\n                \n            except Exception as e:\n                print(f\"Error in evaluation batch {batch_idx}/{num_batches}: {e}\")\n                continue\n    \n    # Calculate metrics\n    avg_loss = total_loss / num_batches\n    token_accuracy = total_correct / total_tokens if total_tokens > 0 else 0\n    sequence_accuracy = correct_sequences / total_sequences if total_sequences > 0 else 0\n    \n    return {\n        \"val_loss\": avg_loss,\n        \"token_accuracy\": token_accuracy,\n        \"sequence_accuracy\": sequence_accuracy\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Question-2: WandB sweeps with val_loss","metadata":{}},{"cell_type":"code","source":"# ---- Sweep Configuration ----\nsweep_config = {\n    'method': 'bayes',\n    'metric': {'name': 'val_loss', 'goal': 'minimize'},\n    'parameters': {\n        'embed_dim': {'values': [16, 32, 64, 256]},\n        'hidden_dim': {'values': [16, 32, 64, 256]},\n        'cell_type': {'values': ['GRU','RNN','LSTM']},\n        'enc_layers': {'values': [1, 2, 3]},\n        'dec_layers': {'values': [1, 2, 3]},\n        'dropout': {'values': [0.2, 0.3]},\n        'learning_rate': {'values': [1e-3, 1e-4]},\n        'batch_size': {'values': [32, 64]},\n        'beam_size': {'values': [1, 3, 5]}\n    }\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---- Helper Functions ----\ndef save_vocab(vocab_path, src_vocab, tgt_vocab):\n    \"\"\"Save vocabularies to JSON files\"\"\"\n    os.makedirs(vocab_path, exist_ok=True)\n    with open(os.path.join(vocab_path, 'src.json'), 'w', encoding='utf-8') as f:\n        json.dump(src_vocab, f, ensure_ascii=False, indent=2)\n    with open(os.path.join(vocab_path, 'tgt.json'), 'w', encoding='utf-8') as f:\n        json.dump(tgt_vocab, f, ensure_ascii=False, indent=2)\n\ndef load_vocab(vocab_path):\n    \"\"\"Load vocabularies from JSON files\"\"\"\n    with open(os.path.join(vocab_path, 'src.json'), 'r', encoding='utf-8') as f:\n        src_vocab = json.load(f)\n    with open(os.path.join(vocab_path, 'tgt.json'), 'r', encoding='utf-8') as f:\n        tgt_vocab = json.load(f)\n    return src_vocab, tgt_vocab\n\n# ---- Simple test method to check for CUDA issues ----\ndef test_cuda():\n    \"\"\"Test if CUDA works correctly\"\"\"\n    print(\"Testing CUDA...\")\n    try:\n        x = torch.rand(10, 10)\n        if torch.cuda.is_available():\n            print(\"CUDA is available\")\n            x = x.cuda()\n            print(\"Successfully moved tensor to CUDA\")\n            y = x + x\n            print(\"Successfully performed CUDA operation\")\n        else:\n            print(\"CUDA is not available, using CPU\")\n        return True\n    except Exception as e:\n        print(f\"CUDA test failed: {e}\")\n        return False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cuda_ok = test_cuda()\n\n# File paths\ntrain_tsv = '/kaggle/input/dakshina-dataset/hindi/hi/lexicons/hi.translit.sampled.train.tsv'\ndev_tsv = '/kaggle/input/dakshina-dataset/hindi/hi/lexicons/hi.translit.sampled.dev.tsv'\nvocab_path = '/kaggle/working/vocab'\nepochs = 10\n\n# Build vocabulary\nprint(\"Building vocabulary...\")\ntrain_dataset = tsvtokenizer(train_tsv, build_vocab=True)\nsrc_vocab, tgt_vocab = train_dataset.src_vocab, train_dataset.tgt_vocab\n\n# Save vocabulary\nsave_vocab(vocab_path, src_vocab, tgt_vocab)\nprint(f\"Vocabulary sizes: Source = {len(src_vocab)}, Target = {len(tgt_vocab)}\")\n\n# Print sample entries from vocabulary\nprint(\"Sample source vocabulary entries:\")\nsample_src = list(src_vocab.items())[:10]\nfor char, idx in sample_src:\n    print(f\"  '{char}': {idx}\")\n\nprint(\"Sample target vocabulary entries:\")\nsample_tgt = list(tgt_vocab.items())[:10]\nfor char, idx in sample_tgt:\n    print(f\"  '{char}': {idx}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize W&B sweep\nsweep_id = wandb.sweep(sweep_config, project='DA6401_Assignment_03')\ndef sweep_run(epochs=10):\n    \"\"\"Function to run for each sweep configuration\"\"\"\n    run = wandb.init()\n    cfg = run.config\n    \n    # Create a descriptive run name\n    run.name = f\"{cfg.cell_type}-e{cfg.embed_dim}-h{cfg.hidden_dim}-enc{cfg.enc_layers}-dec{cfg.dec_layers}-d{cfg.dropout}-lr{cfg.learning_rate}-b{cfg.batch_size}-beam{cfg.beam_size}\"\n    \n    # Set device - force CPU initially if CUDA issues were detected\n    if cuda_ok:\n        try:\n            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        except:\n            device = torch.device('cpu')\n    else:\n        device = torch.device('cpu')\n        \n    print(f\"Using device: {device}\")\n    \n    # Initialize model\n    try:\n        # First create on CPU\n        model = Seq2Seq(cfg, len(src_vocab), len(tgt_vocab))\n        print(\"Model created on CPU, trying to move to device...\")\n        # Then try to move to target device\n        model = model.to(device)\n        print(\"Model successfully moved to device.\")\n    except Exception as e:\n        print(f\"Error initializing model on {device}: {e}\")\n        print(\"Falling back to CPU\")\n        device = torch.device('cpu')\n        model = Seq2Seq(cfg, len(src_vocab), len(tgt_vocab)).to(device)\n    \n    # Load datasets\n    print(\"Loading datasets...\")\n    train_dataset = tsvtokenizer(train_tsv, src_vocab, tgt_vocab)\n    dev_dataset = tsvtokenizer(dev_tsv, src_vocab, tgt_vocab)\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n    dev_loader = DataLoader(dev_dataset, batch_size=cfg.batch_size)\n    print(f\"Loaded {len(train_dataset)} training examples and {len(dev_dataset)} validation examples\")\n        \n    criterion = nn.CrossEntropyLoss(ignore_index=0)  # 0 is padding index\n    optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate)\n    \n    # Training loop\n    best_val_loss = float('inf')\n    step = 0\n        \n    try:\n        for epoch in range(epochs):\n            print(f\"Epoch {epoch+1}/{epochs}\")\n            # Training\n            model.train()\n            total_loss = 0\n            batches = 0\n            \n            for batch_idx, (src, tgt) in enumerate(train_loader):\n                batches += 1\n                src, tgt = src.to(device), tgt.to(device)\n                \n                optimizer.zero_grad()\n                output = model(src, tgt)\n                \n                # Reshape output and target for loss calculation\n                output = output[:, :-1, :].reshape(-1, output.shape[-1])\n                target = tgt[:, 1:].reshape(-1)\n                \n                loss = criterion(output, target)\n                loss.backward()\n                optimizer.step()\n                \n                total_loss += loss.item()\n                \n                # Optionally log every N batches\n                if batch_idx % 500 == 0 and batch_idx > 0:\n                    wandb.log({\n                        'batch_train_loss': loss.item(),\n                    }, step=step + batch_idx)\n            \n            step += batches\n            train_loss = total_loss / batches\n            print(f\"Train loss: {train_loss:.4f}\")\n            \n            # Validation\n            model.eval()\n            total_val_loss = 0\n            val_batches = 0\n            \n            with torch.no_grad():\n                for src, tgt in dev_loader:\n                    val_batches += 1\n                    src, tgt = src.to(device), tgt.to(device)\n                    \n                    output = model(src, tgt)\n                    \n                    # Reshape output and target for loss calculation\n                    output = output[:, :-1, :].reshape(-1, output.shape[-1])\n                    target = tgt[:, 1:].reshape(-1)\n                    \n                    loss = criterion(output, target)\n                    total_val_loss += loss.item()\n            \n            val_loss = total_val_loss / val_batches\n            print(f\"Validation loss: {val_loss:.4f}\")\n            \n            # Log metrics\n            wandb.log({\n                'train_loss': train_loss, \n                'val_loss': val_loss, \n                'epoch': epoch+1,\n            }, step=step)\n                \n            # Save best model - create a NEW artifact each time\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                model_path = f'/kaggle/working/best_model_sweep.pt'\n                \n                # Save the model\n                torch.save({\n                    'model_state_dict': model.state_dict(),\n                    'config': {k: v for k, v in cfg.items()},\n                    'src_vocab_size': len(src_vocab),\n                    'tgt_vocab_size': len(tgt_vocab),\n                    'epoch': epoch+1,\n                    'step': step,\n                    'best_val_loss': best_val_loss,\n                }, model_path)\n                \n                # Create a NEW artifact with a unique name including the epoch\n                artifact_name = f'model-epoch-{epoch+1}'\n                artifact = wandb.Artifact(artifact_name, type='model')\n                artifact.add_file(model_path)\n                run.log_artifact(artifact)\n                \n                print(f\"Saved new best model at {model_path} with validation loss: {val_loss:.4f}\")\n                \n    except Exception as e:\n        print(f\"Error during training: {str(e)}\")\n        import traceback\n        traceback.print_exc()  # This will print the full stack trace","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run the sweep\nwandb.agent(sweep_id, function=sweep_run, count=30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Question-3: Observations based on sweeps.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Question-4: Running on test data, creating predictions vanilla folder, analyze and comment on errors.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the best model from your sweep\ndef load_best_model(model_path, src_vocab, tgt_vocab):\n    checkpoint = torch.load(model_path)\n    \n    # Extract configuration from checkpoint\n    config_dict = checkpoint['config']\n    # Convert to a config object with attributes\n    cfg = type('Config', (), config_dict)\n    \n    # Initialize model with saved config\n    model = Seq2Seq(cfg, len(src_vocab), len(tgt_vocab))\n    model.load_state_dict(checkpoint['model_state_dict'])\n    return model, cfg","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\ndef get_predictions(model, loader, device, src_vocab, tgt_vocab, num_samples=10):\n    \"\"\"\n    Get sample predictions from the model using the generate function.\n    \n    Args:\n        model: Trained Seq2Seq model\n        loader: DataLoader with evaluation data\n        device: Device to run inference on\n        src_vocab: Source vocabulary (char -> idx)\n        tgt_vocab: Target vocabulary (char -> idx)\n        num_samples: Number of samples to collect\n        \n    Returns:\n        List of dictionaries with source, target, prediction, and correctness\n    \"\"\"\n    model.eval()\n    predictions = []\n    \n    # Create reverse vocabulary mappings\n    idx_to_src = {idx: char for char, idx in src_vocab.items()}\n    idx_to_tgt = {idx: char for char, idx in tgt_vocab.items()}\n    \n    # Get indices for special tokens\n    eos_idx = tgt_vocab.get('<eos>', 3)\n    sos_idx = tgt_vocab.get('<sos>', 2)\n    \n    with torch.no_grad():\n        for src, tgt in loader:\n            # Move data to device\n            src, tgt = src.to(device), tgt.to(device)\n            \n            # Generate predictions using the specialized generate function\n            generated_sequences = generate(\n                model,\n                src,\n                max_len=tgt.size(1),  # Use same length as target sequences\n                eos_idx=eos_idx,\n                sos_idx=sos_idx,\n                device=device\n            )\n            \n            # Decode all sequences\n            src_texts = decode_batch(src, idx_to_src, eos_idx=eos_idx)\n            tgt_texts = decode_batch(tgt, idx_to_tgt, eos_idx=eos_idx)\n            pred_texts = decode_batch(generated_sequences, idx_to_tgt, eos_idx=eos_idx)\n            \n            # Process batch\n            for i in range(src.size(0)):\n                # Check if prediction is correct (after removing any special tokens)\n                is_correct = (tgt_texts[i] == pred_texts[i])\n                \n                predictions.append({\n                    'source': src_texts[i],\n                    'target': tgt_texts[i],\n                    'prediction': pred_texts[i],\n                    'is_correct': is_correct\n                })\n                \n                if len(predictions) >= num_samples:\n                    return predictions\n    \n    return predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def retrain_model(model_path, train_tsv, dev_tsv, vocab_path, epochs=20, max_len=32, early_stop_patience=5):\n    \"\"\"\n    Retrain the best model from scratch using the best hyperparameters\n    from the sweep.\n    \n    Args:\n        model_path: Path to the best model checkpoint\n        train_tsv: Path to the training data\n        dev_tsv: Path to the validation data\n        vocab_path: Path to save/load vocabulary\n        epochs: Number of epochs to train for\n        max_len: Maximum sequence length\n        early_stop_patience: Number of epochs to wait without improvement before stopping\n        \n    Returns:\n        model: The trained model\n        best_val_loss: The best validation loss achieved\n        src_vocab: Source vocabulary\n        tgt_vocab: Target vocabulary\n    \"\"\"\n    # Set up device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Load vocabulary from JSON files\n    try:\n        print(\"Loading saved vocabulary...\")\n        with open(f\"{vocab_path}_src.json\", 'r', encoding='utf-8') as f:\n            src_vocab = json.load(f)\n        with open(f\"{vocab_path}_tgt.json\", 'r', encoding='utf-8') as f:\n            tgt_vocab = json.load(f)\n        print(\"Vocabulary loaded successfully\")\n    except Exception as e:\n        print(f\"Error loading vocabulary: {e}\")\n        print(\"Building vocabulary from training data...\")\n        train_dataset = tsvtokenizer(train_tsv, max_len=max_len, build_vocab=True)\n        src_vocab, tgt_vocab = train_dataset.src_vocab, train_dataset.tgt_vocab\n        \n        # Save vocabulary\n        try:\n            print(\"Saving vocabulary...\")\n            os.makedirs(os.path.dirname(vocab_path), exist_ok=True)\n            with open(f\"{vocab_path}_src.json\", 'w', encoding='utf-8') as f:\n                json.dump(src_vocab, f, ensure_ascii=False, indent=2)\n            with open(f\"{vocab_path}_tgt.json\", 'w', encoding='utf-8') as f:\n                json.dump(tgt_vocab, f, ensure_ascii=False, indent=2)\n            print(\"Vocabulary saved successfully\")\n        except Exception as e:\n            print(f\"Error saving vocabulary: {e}\")\n    \n    print(f\"Vocabulary sizes: Source = {len(src_vocab)}, Target = {len(tgt_vocab)}\")\n    \n    # Load the best model configuration\n    print(f\"Loading best model configuration from: {model_path}\")\n    try:\n        checkpoint = torch.load(model_path, map_location=device)\n        config_dict = checkpoint['config']\n        print(\"Successfully loaded checkpoint\")\n    except Exception as e:\n        print(f\"Error loading checkpoint: {e}\")\n        raise\n    \n    # Initialize a new run with the same config\n    try:\n        run = wandb.init(project='DA6401_Assignment_03_Retrain', \n                        config=config_dict,\n                        name=f\"Retrain_{os.path.basename(model_path).replace('best_model_', '').replace('.pt', '')}\")\n        \n        cfg = run.config\n    except Exception as e:\n        print(f\"Error initializing wandb: {e}. Continuing without logging.\")\n        cfg = config_dict\n    \n    # Create a Config object with attributes for model initialization\n    class Config:\n        def __init__(self, config_dict):\n            for key, value in config_dict.items():\n                setattr(self, key, value)\n    \n    config_obj = Config(config_dict)\n    \n    # Print configuration\n    print(\"Training with the following configuration:\")\n    for key, value in config_dict.items():\n        print(f\"  {key}: {value}\")\n    \n    # Initialize model with the best hyperparameters\n    print(\"Initializing model with best hyperparameters\")\n    model = Seq2Seq(config_obj, len(src_vocab), len(tgt_vocab)).to(device)\n    \n    # Load datasets\n    print(\"Loading datasets...\")\n    train_dataset = tsvtokenizer(train_tsv, src_vocab, tgt_vocab, max_len=max_len)\n    dev_dataset = tsvtokenizer(dev_tsv, src_vocab, tgt_vocab, max_len=max_len)\n    \n    # Create data loaders\n    batch_size = config_dict.get('batch_size', 64)  # Use default if not available\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    dev_loader = DataLoader(dev_dataset, batch_size=batch_size)\n    print(f\"Loaded {len(train_dataset)} training examples and {len(dev_dataset)} validation examples\")\n    \n    # Initialize loss function and optimizer\n    criterion = nn.CrossEntropyLoss(ignore_index=0)  # 0 is padding index\n    lr = config_dict.get('learning_rate', 0.001)  # Use default if not available\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    # Add learning rate scheduler\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, 'min', patience=2, factor=0.5, verbose=True\n    )\n    \n    # Training loop\n    best_val_loss = float('inf')\n    best_model_state = None\n    patience_counter = 0\n    step = 0\n    start_time = time.time()\n    \n    # Track some sample predictions\n    print(\"Collecting initial sample predictions...\")\n    samples = get_predictions(model, dev_loader, device, src_vocab, tgt_vocab, num_samples=5)\n    for i, sample in enumerate(samples):\n        print(f\"Sample {i+1}:\")\n        print(f\"  Source: {sample['source']}\")\n        print(f\"  Target: {sample['target']}\")\n        print(f\"  Prediction: {sample['prediction']}\")\n        print(f\"  Correct: {sample['is_correct']}\")\n    \n    for epoch in range(epochs):\n        epoch_start = time.time()\n        print(f\"Epoch {epoch+1}/{epochs}\")\n        \n        # Training\n        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n        \n        # Validation\n        val_result = eval_model(model, dev_loader, criterion, device)\n        val_loss = val_result['val_loss']\n        \n        # Update learning rate scheduler\n        scheduler.step(val_loss)\n        \n        # Calculate epoch time\n        epoch_time = time.time() - epoch_start\n        \n        print(f\"Epoch {epoch+1} complete in {epoch_time:.2f}s\")\n        print(f\"  Train loss: {train_loss:.4f}\")\n        print(f\"  Validation loss: {val_loss:.4f}\")\n        print(f\"  Current learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n        \n        # Log metrics if wandb is available\n        try:\n            wandb.log({\n                'train_loss': train_loss, \n                'val_loss': val_loss, \n                'epoch': epoch+1,\n                'learning_rate': optimizer.param_groups[0]['lr'],\n                'epoch_time': epoch_time\n            }, step=step + epoch)\n        except Exception as e:\n            print(f\"Error logging to wandb: {e}. Continuing without logging.\")\n        \n        # Save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_model_state = model.state_dict().copy()\n            model_path_epoch = f'/kaggle/working/final_retrained_model.pt'\n            \n            # Save the model\n            torch.save({\n                'model_state_dict': model.state_dict(),\n                'config': config_dict,\n                'src_vocab_size': len(src_vocab),\n                'tgt_vocab_size': len(tgt_vocab),\n                'epoch': epoch+1,\n                'step': step + epoch,\n                'best_val_loss': best_val_loss,\n            }, model_path_epoch)\n            \n            # Log artifact if wandb is available\n            try:\n                # Create a NEW artifact with a unique name\n                artifact_name = f'retrained-model-epoch-{epoch+1}'\n                artifact = wandb.Artifact(artifact_name, type='model')\n                artifact.add_file(model_path_epoch)\n                run.log_artifact(artifact)\n            except Exception as e:\n                print(f\"Error logging artifact to wandb: {e}. Continuing without logging.\")\n            \n            print(f\"Saved new best model with validation loss: {val_loss:.4f}\")\n            \n            # Reset patience counter\n            patience_counter = 0\n            \n            # Get and print some sample predictions\n            if (epoch + 1) % 5 == 0 or epoch == epochs - 1:\n                print(\"Collecting sample predictions...\")\n                samples = get_predictions(model, dev_loader, device, src_vocab, tgt_vocab, num_samples=5)\n                for i, sample in enumerate(samples):\n                    print(f\"Sample {i+1}:\")\n                    print(f\"  Source: {sample['source']}\")\n                    print(f\"  Target: {sample['target']}\")\n                    print(f\"  Prediction: {sample['prediction']}\")\n                    print(f\"  Correct: {sample['is_correct']}\")\n        else:\n            # Increment patience counter if no improvement\n            patience_counter += 1\n            if patience_counter >= early_stop_patience:\n                print(f\"Early stopping triggered after {epoch+1} epochs without improvement\")\n                break\n    \n    total_time = time.time() - start_time\n    print(f\"Training completed in {total_time:.2f}s\")\n    print(f\"Best validation loss: {best_val_loss:.4f}\")\n    \n    # If we have a best model state, load it back\n    if best_model_state:\n        model.load_state_dict(best_model_state)\n    \n    # Calculate final accuracy on dev set\n    print(\"Evaluating model on development set...\")\n    samples = get_predictions(model, dev_loader, device, src_vocab, tgt_vocab, num_samples=100)\n    correct = sum(1 for sample in samples if sample['is_correct'])\n    accuracy = correct / len(samples) if samples else 0\n    print(f\"Development set accuracy: {accuracy:.4f} ({correct}/{len(samples)})\")\n    \n    try:\n        wandb.log({'final_accuracy': accuracy})\n        wandb.finish()\n    except Exception as e:\n        print(f\"Error finishing wandb run: {e}\")\n    \n    # Test with a few examples using the transliterate function\n    print(\"\\nTesting with a few examples:\")\n    test_examples = [\"hello\", \"world\", \"python\", \"transliteration\"]\n    for example in test_examples:\n        result = transliterate(model, example, src_vocab, tgt_vocab, device, max_len=max_len)\n        print(f\"  '{example}' -> '{result}'\")\n    \n    return model, best_val_loss, src_vocab, tgt_vocab","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_model_path = \"/kaggle/working/best_model_sweep.pt\"\ntest_tsv = '/kaggle/input/dakshina-dataset/hindi/hi/lexicons/hi.translit.sampled.test.tsv'\nvocab_path = '/kaggle/working/vocab'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Retraining model with best hyperparameters...\")\nepochs = 50  # Increase for better results\nretrained_model, best_val_loss,src_vocab,tgt_vocab = retrain_model(\n    best_model_path, train_tsv, dev_tsv, vocab_path, epochs=epochs\n)\nwandb.init()\nconfig_dict = {k: v for k, v in wandb.config.items()}\nretrained_model_path = '/kaggle/working/final_retrained_model.pt'\ntorch.save({\n    'model_state_dict': retrained_model.state_dict(),\n    'val_loss': best_val_loss,\n    'src_vocab_size': len(src_vocab),\n    'tgt_vocab_size': len(tgt_vocab),\n    'config': config_dict,  # Save the current config\n}, retrained_model_path)\nprint(f\"Retrained model saved to {retrained_model_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_model_for_inference(model_path, src_vocab, tgt_vocab):\n    \"\"\"\n    Load a trained model for inference.\n    \n    Args:\n        model_path: Path to the model checkpoint\n        src_vocab: Source vocabulary\n        tgt_vocab: Target vocabulary\n        \n    Returns:\n        model: The loaded model on the appropriate device\n        config: Configuration object\n    \"\"\"\n    # Set up device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Load checkpoint\n    checkpoint = torch.load(model_path, map_location=device)\n    \n    # Extract configuration\n    config_dict = checkpoint['config']\n    \n    # Create a Config object with attributes for model initialization\n    class Config:\n        def __init__(self, config_dict):\n            for key, value in config_dict.items():\n                setattr(self, key, value)\n    \n    config_obj = Config(config_dict)\n    \n    # Print configuration\n    print(\"Model configuration:\")\n    for key, value in config_dict.items():\n        print(f\"  {key}: {value}\")\n    \n    # Initialize model with the loaded configuration\n    model = Seq2Seq(config_obj, len(src_vocab), len(tgt_vocab)).to(device)\n    \n    # Load the model weights\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    # Set model to evaluation mode\n    model.eval()\n    \n    return model, config_obj\n\n# Usage example:\nmodel, cfg = load_model_for_inference(retrained_model_path, src_vocab, tgt_vocab)\nprint(f\"Model loaded successfully on {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load vocabulary\nwith open(os.path.join(vocab_path, 'src.json'), 'r', encoding='utf-8') as f:\n    src_vocab = json.load(f)\nwith open(os.path.join(vocab_path, 'tgt.json'), 'r', encoding='utf-8') as f:\n    tgt_vocab = json.load(f)\n\n# Load test dataset\ntest_dataset = tsvtokenizer(test_tsv, src_vocab, tgt_vocab)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load best model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ncheckpoint = torch.load(retrained_model_path, map_location=device)\nconfig_dict = checkpoint.get('config', {})\n# Convert to a config object with attributes\ncfg = type('Config', (), config_dict)\nmodel = Seq2Seq(cfg, len(src_vocab), len(tgt_vocab))\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel = model.to(device)\nmodel.eval() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate model using exact_match_accuracy\naccuracy = exact_match_accuracy(model, test_loader, device, tgt_vocab)\nprint(f\"Test accuracy: {accuracy:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(compute_metrics(model, test_loader, device,src_vocab, tgt_vocab))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get sample predictions\nsample_predictions = get_predictions(model, test_loader, device, src_vocab, tgt_vocab, num_samples=20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nfrom matplotlib.font_manager import FontProperties\nimport os\nimport urllib.request\nfrom pathlib import Path\n\n\n# Get the Hindi font\nhindi_font = FontProperties(fname='/kaggle/input/ttf-fonts/NotoSansDevanagari-VariableFont_wdth,wght.ttf', size=18)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display sample predictions in a nice grid\nplt.figure(figsize=(15, 10))\nfor i, pred in enumerate(sample_predictions[:9]):\n    plt.subplot(3, 3, i + 1)\n    \n    # Create text for display\n    text = f\"Input: {pred['source']}\\n\"\n    text += f\"Target: {pred['target']}\\n\"\n    text += f\"Prediction: {pred['prediction']}\\n\"\n    text += f\"Status: {'True' if pred['is_correct'] else 'False'}\"\n    \n    # Set background color based on correctness\n    bg_color = '#e6ffe6' if pred['is_correct'] else '#ffe6e6'\n    \n    # Create a text box\n    plt.text(0.5, 0.5, text, ha='center', va='center', wrap=True, \n             bbox=dict(facecolor=bg_color, alpha=0.8, boxstyle='round,pad=1'),fontproperties=hindi_font)\n    plt.axis('off')\n\nplt.tight_layout()\nplt.savefig('sample_predictions.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_all_predictions(model, loader, device, src_vocab, tgt_vocab, output_file):\n    \"\"\"\n    Save all predictions to a file using the generate function.\n    \n    Args:\n        model: Trained Seq2Seq model\n        loader: DataLoader with evaluation data\n        device: Device to run inference on\n        src_vocab: Source vocabulary (char -> idx)\n        tgt_vocab: Target vocabulary (char -> idx)\n        output_file: Path to output JSON file\n    \"\"\"\n    model.eval()\n    all_predictions = []\n    \n    # Create reverse vocabulary mappings\n    idx_to_src = {idx: char for char, idx in src_vocab.items()}\n    idx_to_tgt = {idx: char for char, idx in tgt_vocab.items()}\n    \n    # Get indices for special tokens\n    eos_idx = tgt_vocab.get('<eos>', 3)\n    sos_idx = tgt_vocab.get('<sos>', 2)\n    \n    with torch.no_grad():\n        for src, tgt in loader:\n            # Move data to device\n            src, tgt = src.to(device), tgt.to(device)\n            \n            # Generate predictions using the specialized generate function\n            generated_sequences = generate(\n                model,\n                src,\n                max_len=tgt.size(1),  # Use same length as target sequences\n                eos_idx=eos_idx,\n                sos_idx=sos_idx,\n                device=device\n            )\n            \n            # Decode all sequences\n            src_texts = decode_batch(src, idx_to_src, eos_idx=eos_idx)\n            tgt_texts = decode_batch(tgt, idx_to_tgt, eos_idx=eos_idx)\n            pred_texts = decode_batch(generated_sequences, idx_to_tgt, eos_idx=eos_idx)\n            \n            # Process batch\n            for i in range(src.size(0)):\n                # Check if prediction is correct (after removing any special tokens)\n                is_correct = (tgt_texts[i] == pred_texts[i])\n                \n                all_predictions.append({\n                    'source': src_texts[i],\n                    'target': tgt_texts[i],\n                    'prediction': pred_texts[i],\n                    'is_correct': is_correct\n                })\n    \n    # Save to file\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(all_predictions, f, ensure_ascii=False, indent=2)\n    \n    return all_predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save all predictions\nall_predictions = save_all_predictions(model, test_loader, device, src_vocab, tgt_vocab, \n                                      'predictions_vanilla/test_predictions.json')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\n# Analyze errors\ndef analyze_errors(predictions):\n    # Initialize counters\n    total = len(predictions)\n    correct = sum(1 for p in predictions if p['target'] == p['prediction'])\n    \n    # Calculate error rate by sequence length\n    length_errors = {}\n    for p in predictions:\n        length = len(p['target'])\n        if length not in length_errors:\n            length_errors[length] = {'total': 0, 'errors': 0}\n        \n        length_errors[length]['total'] += 1\n        if p['target'] != p['prediction']:\n            length_errors[length]['errors'] += 1\n    \n    # Convert to error rates\n    length_error_rates = {\n        length: data['errors'] / data['total'] * 100 \n        for length, data in length_errors.items() if data['total'] > 0\n    }\n    \n    # Analyze character-level errors\n    char_errors = []\n    for p in predictions:\n        if p['target'] != p['prediction']:\n            # Find character-level differences\n            min_len = min(len(p['target']), len(p['prediction']))\n            for i in range(min_len):\n                if p['target'][i] != p['prediction'][i]:\n                    char_errors.append((p['target'][i], p['prediction'][i]))\n    \n    # Count character error frequencies\n    char_error_counts = Counter(char_errors)\n    \n    return {\n        'accuracy': correct / total * 100,\n        'length_error_rates': length_error_rates,\n        'char_error_counts': char_error_counts\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Analyze errors\nerror_analysis = analyze_errors(all_predictions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot error rate by sequence length\nplt.figure(figsize=(10, 6))\nlengths = sorted(error_analysis['length_error_rates'].keys())\nerror_rates = [error_analysis['length_error_rates'][length] for length in lengths]\n\nplt.bar(lengths, error_rates)\nplt.xlabel('Sequence Length')\nplt.ylabel('Error Rate (%)')\nplt.title('Error Rate by Sequence Length')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.savefig('error_by_length.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot top character-level errors\nplt.figure(figsize=(12, 6))\ntop_errors = error_analysis['char_error_counts'].most_common(10)\nerror_chars = [f\"{true}->{pred}\" for (true, pred) in [err[0] for err in top_errors]]\nerror_counts = [count for _, count in top_errors]\n\nplt.bar(error_chars, error_counts)\nplt.xlabel('Character Error (True->Predicted)',fontproperties=hindi_font)\nplt.ylabel('Count',fontproperties=hindi_font)\nplt.title('Top 10 Character-Level Errors',fontproperties=hindi_font)\nplt.xticks(rotation=45,fontproperties=hindi_font)\nplt.tight_layout()\nplt.savefig('top_char_errors.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Overall accuracy: {error_analysis['accuracy']:.2f}%\")\nprint(\"Top 5 character-level errors:\")\nfor i, ((true_char, pred_char), count) in enumerate(error_analysis['char_error_counts'].most_common(5)):\n    print(f\"  {i+1}. '{true_char}' -> '{pred_char}': {count} times\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}